# -*- coding: utf-8 -*-
"""voicegpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qY4G249zGOyd3UGP3bbplXTw-rjMeBEo
"""

! pip install gradio openai gtts pydub numpy requests groq openai-whisper
!apt-get install -y ffmpeg

import os
import gradio as gr
import whisper
from gtts import gTTS
from IPython.display import Audio
from groq import Groq
import openai
import numpy as np

import os
import gradio as gr
import whisper
from gtts import gTTS
from groq import Groq, GroqError
from typing import Tuple, Union

# Initialize Whisper model
model = whisper.load_model("base")

# Initialize Groq API client with the API key directly
api_key = "groq_api_key"
try:
    client = Groq(api_key=api_key)
except Exception as e:
    raise RuntimeError(f"Failed to initialize Groq client: {e}")

def transcribe_and_respond(audio: str) -> Tuple[str, Union[str, None]]:
    try:
        # Step 1: Transcribe the audio using Whisper
        transcription = model.transcribe(audio)
        user_input = transcription['text']

        # Step 2: Generate a response using Groq API and LLaMA model
        try:
            chat_completion = client.chat.completions.create(
                messages=[{"role": "user", "content": user_input}],
                model="llama3-8b-8192",
            )
            response_text = chat_completion.choices[0].message.content
        except GroqError as e:
            return f"Error in Groq API call: {e}", None

        # Step 3: Convert the response text to speech using gTTS
        tts = gTTS(response_text)
        audio_path = "response.mp3"
        tts.save(audio_path)

        return response_text, audio_path

    except FileNotFoundError:
        return "Error: Audio file not found.", None
    except whisper.WhisperError as e:
        return f"Error in transcription: {e}", None
    except Exception as e:
        return f"An unexpected error occurred: {e}", None

# Gradio interface for real-time interaction
interface = gr.Interface(
    fn=transcribe_and_respond,
    inputs=gr.Audio(type="filepath"),
    outputs=[gr.Textbox(label="Response"), gr.Audio(label="Voice Response")],
    live=True
)

interface.launch()